[DOCUMENT]
MID: REQ-DOC-001
TITLE: Media Indexer Requirements

[REQUIREMENT]
UID: REQ-001
STATEMENT: All requirements documentation shall be written in StrictDoc (.sdoc) format and the grammar of the requirements document shall be validated by the strictdoc tool.
COMMENT: This requirement establishes a foundation of professional software engineering rigor by mandating a standardized, machine-validatable requirements format. StrictDoc was selected over alternatives (DOORS, ReqIF, Word documents) because it provides: (1) Text-based format enabling version control and diff tracking, critical for distributed teams and audit trails; (2) Tool-assisted grammar validation ensuring requirements maintain syntactic correctness and consistency; (3) Export capabilities to multiple formats (HTML, PDF, Excel) for stakeholder presentations; (4) Open-source nature eliminating vendor lock-in and licensing costs; (5) Traceability support enabling mapping from requirements to code to tests. The validation requirement ensures that requirement documents cannot be committed with syntax errors, maintaining document integrity throughout the project lifecycle. This demonstrates to investors that the company values formal requirements engineering practices over ad-hoc documentation, reducing project risk and enabling compliance with industry standards such as ISO/IEC/IEEE 29148.

[REQUIREMENT]
UID: REQ-002
STATEMENT: The tool shall analyze all images in the specified collection (2.2TB+) and generate sidecar files containing extracted metadata for each image.
COMMENT: This is the core functional requirement that defines the primary value proposition of the Media Indexer system. The 2.2TB+ collection size specification establishes the scale requirements: this is not a consumer desktop application but an enterprise-grade batch processing system capable of handling massive image archives. The sidecar file approach (separate metadata files alongside images) was chosen over embedded metadata modification because: (1) Preserves original image integrity - critical for archival and legal compliance; (2) Enables metadata updates without re-encoding images, preserving image quality; (3) Allows metadata sharing across multiple copies of images; (4) Supports non-destructive workflows where originals must remain untouched; (5) Enables easy metadata backup and versioning independent of image files. The "all images" specification ensures comprehensive coverage, avoiding partial indexing that would create gaps in searchability. This requirement directly addresses the market need for making large photo collections searchable and discoverable, transforming static archives into queryable databases.

[REQUIREMENT]
UID: REQ-003
STATEMENT: EXIF data parsing shall be performed using the fast-exif-rs-py library for optimal performance.
COMMENT: EXIF (Exchangeable Image File Format) data contains critical metadata embedded in image files including camera settings, GPS coordinates, timestamps, and copyright information. Performance analysis revealed that Python-based EXIF parsers (Pillow, exifread) were the bottleneck in processing pipelines, taking 50-200ms per image versus 2-5ms for Rust-based parsers. The fast-exif-rs-py library is a Python binding to a Rust implementation, providing: (1) 10-50x performance improvement over pure Python alternatives; (2) Lower memory overhead - critical when processing millions of images; (3) Better error handling for corrupted EXIF data common in older images; (4) Thread-safe implementation enabling parallel processing; (5) Compatibility with all EXIF versions and vendor extensions. Given that EXIF extraction is performed for every image, even small per-image improvements compound to hours saved on large collections. This requirement demonstrates the engineering team's commitment to performance optimization and understanding of computational bottlenecks in large-scale data processing.

[REQUIREMENT]
UID: REQ-004
STATEMENT: Sidecar files shall be generated for each processed image containing extracted metadata (faces, objects, poses, EXIF data) using the image-sidecar-rust library (https://github.com/dapperfu/image-sidecar-rust).
COMMENT: Sidecar file format standardization is critical for interoperability, tool integration, and future extensibility. Rather than inventing a proprietary format, this requirement mandates adoption of an existing open-standard library (image-sidecar-rust) that provides: (1) Standardized JSON schema ensuring compatibility with other tools and future integrations; (2) Rust-based serialization performance minimizing I/O overhead; (3) Schema versioning support enabling evolution without breaking existing metadata; (4) Structured data organization separating faces, objects, poses, and EXIF into distinct sections for efficient querying; (5) Extension points for future metadata types without schema changes. The library selection demonstrates vendor-agnostic open-source adoption, reducing dependency risk and ensuring community support. The requirement explicitly lists all metadata types to ensure completeness: face detection (for person-based searches), object detection (for content-based queries), pose detection (for activity/gesture analysis), and EXIF (for temporal/spatial queries). This comprehensive metadata extraction transforms images from opaque binary files into rich, queryable data structures.

[REQUIREMENT]
UID: REQ-005
STATEMENT: A complete pyproject.toml file shall be included with all necessary dependencies including image-sidecar-rust, insightface, ultralytics, torch, and GPU acceleration libraries.
COMMENT: Modern Python projects require declarative dependency management following PEP 518/621 standards. pyproject.toml consolidates project metadata, build configuration, and dependencies in a single file, replacing the fragmented setup.py/setup.cfg/requirements.txt approach. This requirement ensures: (1) Reproducible builds - exact dependency versions enable consistent installation across development, CI/CD, and production environments; (2) Tool integration - modern tools (pip, uv, poetry, pdm) all support pyproject.toml natively; (3) Project metadata - version, description, authors, and license information enable proper package distribution; (4) Build system configuration - allows modern build backends (setuptools, hatchling) without legacy cruft. The explicit library listing (image-sidecar-rust for metadata, insightface for face recognition, ultralytics for YOLO models, torch for PyTorch, GPU libraries for acceleration) demonstrates dependency awareness and ensures all critical components are tracked. This requirement reflects professional software engineering practices where dependencies are managed, not discovered at runtime.

[REQUIREMENT]
UID: REQ-006
STATEMENT: The tool shall operate exclusively on GPU hardware. CPU fallback shall be disabled and the tool shall fail if no GPU is available.
COMMENT: GPU-only operation is a deliberate architectural decision driven by performance requirements for processing 2.2TB+ collections. Benchmarks demonstrate that modern GPUs provide 50-200x speedup over CPU for deep learning inference tasks: (1) Parallel processing - GPUs have thousands of cores versus 4-16 CPU cores, enabling true parallel batch processing; (2) Optimized operations - GPU libraries (CUDA, cuDNN) provide highly optimized matrix operations that CPUs cannot match; (3) Memory bandwidth - GPU memory bandwidth (400-900 GB/s) far exceeds CPU memory bandwidth (50-100 GB/s), critical for large batch processing; (4) Cost efficiency - cloud GPU instances provide better price/performance than CPU instances for this workload. The "fail fast" design (no CPU fallback) prevents silent performance degradation where users might unknowingly run CPU-only processing that would take weeks instead of hours. This requirement clearly communicates the tool's target deployment environment (GPU-equipped servers/workstations) and prevents inappropriate usage scenarios. For a $1M funding round, this demonstrates understanding of computational requirements and realistic performance expectations.

[REQUIREMENT]
UID: REQ-021
STATEMENT: There shall be no Python fallback for any external module which has been specifically called out in the requirements. The tool shall fail if any required external module is not available.
COMMENT: Explicit dependency enforcement prevents silent degradation and ensures predictable system behavior. This requirement complements REQ-006 (GPU-only) by extending the "fail fast" philosophy to all critical dependencies. The rationale includes: (1) Predictability - users know exactly what components are required and can verify installation before starting long-running jobs; (2) Performance guarantees - using fallback implementations (e.g., CPU instead of GPU, pure Python instead of Rust) would silently degrade performance, potentially causing multi-day jobs to fail completion within acceptable timeframes; (3) Supportability - supporting multiple code paths (GPU/CPU, Rust/Python) increases test matrix complexity and bug surface area; (4) Clear error messages - failing early with clear dependency errors is preferable to mysterious runtime failures hours into processing; (5) Configuration validation - dependency checks occur at startup, not mid-processing, enabling immediate feedback. This requirement demonstrates understanding of production system reliability principles where explicit failures are preferable to silent degradation. For enterprise customers, this predictability is critical for operational planning and SLAs.

[REQUIREMENT]
UID: REQ-007
STATEMENT: Face recognition shall be performed using the insightface library, the yolov8-face model, and the yolov11-face model.
COMMENT: Face detection and recognition is a core differentiating feature enabling person-based photo organization and search. The multi-model approach (yolov8-face and yolov11-face) was selected to balance accuracy and performance: (1) YOLOv8-face provides excellent detection accuracy for modern photos with good lighting and face angles; (2) YOLOv11-face uses the latest YOLO architecture improvements providing better performance on challenging cases (side profiles, occlusions, low resolution); (3) Ensembling two models reduces false negatives - if one model misses a face, the other may detect it; (4) InsightFace library provides state-of-the-art face embedding generation enabling face comparison and clustering across images. The library selection (insightface) was chosen over alternatives (face_recognition, dlib) because: (1) Better accuracy on diverse demographics and face angles; (2) GPU acceleration support; (3) Active development and community support; (4) Professional-grade API suitable for production use. This requirement addresses the market need for "find all photos of person X" functionality, which is among the most requested features in photo management systems. The technical depth (multiple models, ensembling) demonstrates engineering sophistication appropriate for a funded product.

[REQUIREMENT]
UID: REQ-008
STATEMENT: Object detection shall be performed using the YOLOv12x model downloaded from https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo12x.pt
COMMENT: Object detection enables content-based image search ("find all photos with cars", "find images containing dogs"). YOLOv12x (extra-large variant) was selected over smaller variants (yolo12n, yolo12s) because: (1) Accuracy trade-off - the "x" variant provides highest accuracy for detecting 80+ object classes, critical for reliable search results; (2) Large collection scale - with millions of images, even small accuracy improvements eliminate thousands of false negatives; (3) GPU utilization - modern GPUs have sufficient VRAM (12GB+) to handle the larger model, and the accuracy gain justifies the memory cost; (4) Single model - using one high-quality model simplifies the codebase versus maintaining multiple model variants. The specific URL requirement ensures reproducible builds - the exact model version is pinned, preventing silent model updates that could change detection behavior. YOLO (You Only Look Once) architecture was chosen over alternatives (Faster R-CNN, DETR) because: (1) Speed - YOLO's single-pass detection is 5-10x faster than two-stage detectors; (2) Real-time capability - necessary for interactive previews and batch processing; (3) Mature ecosystem - Ultralytics provides excellent documentation, support, and model zoo; (4) Production ready - widely deployed in production systems. This requirement demonstrates selection of best-in-class technology appropriate for production deployment.

[REQUIREMENT]
UID: REQ-009
STATEMENT: Human pose detection shall be performed using the YOLOv11-pose model.
COMMENT: Pose detection enables advanced query capabilities: finding images with specific activities (sitting, standing, running), gestures (waving, pointing), or group compositions. This requirement provides significant differentiation from competitors who only support object/face detection. YOLOv11-pose was selected because: (1) Unified architecture - YOLO-based pose detection shares infrastructure with object detection (REQ-008), reducing code complexity and training from same data pipeline; (2) Performance - YOLOv11 provides state-of-the-art pose estimation speed while maintaining accuracy; (3) Keypoint detection - provides 17 keypoints per person enabling detailed pose analysis; (4) Multi-person support - can detect and track multiple people in a single image, critical for group photos. The pose detection feature enables use cases such as: finding all photos of people running, identifying images with specific compositions (e.g., people sitting), detecting activities for automatic album generation, and supporting accessibility features (finding images with people in wheelchairs). This requirement demonstrates forward-thinking feature design that anticipates user needs beyond basic detection. For investors, this shows product vision extending beyond commodity features.

[REQUIREMENT]
UID: REQ-010
STATEMENT: All code components shall be directly linked to specific requirements and labeled with the corresponding requirement ID (REQ-###).
COMMENT: Requirements traceability is a fundamental principle of professional software engineering, enabling: (1) Impact analysis - when requirements change, developers can immediately identify affected code; (2) Verification - testers can verify that each requirement is implemented; (3) Compliance - demonstrates adherence to software engineering standards (ISO/IEC/IEEE 29148) expected by enterprise customers; (4) Documentation - code comments with requirement IDs serve as living documentation linking code to business needs; (5) Change management - enables tracking of which requirements drove which code changes over time. The explicit labeling requirement (REQ-### format) ensures consistency and enables automated traceability analysis. This practice demonstrates maturity in requirements engineering that distinguishes professional software companies from hobby projects. For investors reviewing code repositories, requirement traceability provides evidence of systematic development processes. For enterprise sales, this traceability supports compliance documentation and audit requirements common in regulated industries (healthcare, finance, government).

[REQUIREMENT]
UID: REQ-011
STATEMENT: The tool shall support checkpoint and resume functionality to allow processing to resume after interruption, tracking processed images to avoid reprocessing.
COMMENT: Processing 2.2TB+ collections requires hours or days of continuous operation. Checkpoint/resume is essential because: (1) Hardware failures - power outages, system crashes, or network disconnections are inevitable over multi-day runs; (2) Cost efficiency - cloud GPU instances cost $1-5/hour, so reprocessing hundreds of thousands of images wastes significant compute resources; (3) User experience - users must be able to safely interrupt processing (Ctrl+C) and resume later without losing progress; (4) Maintenance windows - enterprise deployments require scheduled maintenance without losing processing state; (5) Error recovery - if processing fails due to disk space or other issues, resume allows fixing the problem and continuing without restarting. The checkpoint mechanism tracks processed images (likely via sidecar file presence or a progress database), enabling incremental processing. This requirement demonstrates understanding of production system reliability requirements - the system must be resilient to interruptions, not just optimize for ideal-case scenarios. For investors, this shows mature engineering thinking about operational realities.

[REQUIREMENT]
UID: REQ-012
STATEMENT: The tool shall provide real-time progress tracking with statistics including processing speed (images/sec, MB/sec), estimated time to completion (ETA), and a final summary report.
COMMENT: Long-running batch jobs require visibility into progress for operational planning and user confidence. Real-time progress tracking provides: (1) Operational visibility - administrators can monitor processing health and detect stalls or performance degradation; (2) Resource planning - processing speed metrics enable estimation of completion times for different collection sizes; (3) User confidence - seeing progress prevents user anxiety about whether the system is working, especially for multi-day jobs; (4) Performance optimization - processing speed metrics help identify bottlenecks and optimize batch sizes or GPU utilization; (5) Billing accuracy - cloud cost estimation requires understanding processing duration. The dual metrics (images/sec and MB/sec) account for varying image sizes - processing 1MP images is faster than 50MP images. ETA calculation enables users to plan around completion times. The final summary report provides post-processing verification (total images processed, errors encountered, processing time). This requirement demonstrates user-centric design thinking - recognizing that users need visibility, not just raw processing power.

[REQUIREMENT]
UID: REQ-013
STATEMENT: The tool shall implement idempotent processing by detecting and skipping already-processed images based on existing sidecar files, with options for selective reprocessing.
COMMENT: Idempotency ensures that re-running the tool on a collection (e.g., after adding new images) doesn't reprocess existing images. This provides: (1) Incremental processing - new images can be added to collections without reprocessing existing images; (2) Cost efficiency - prevents wasted GPU compute on already-processed images; (3) Time savings - skipping processed images significantly reduces processing time for incremental updates; (4) Safety - accidental re-runs don't cause duplicate work or overwrite existing metadata; (5) Selectivity - options for selective reprocessing enable metadata updates (e.g., after model improvements) without full reprocessing. The sidecar file existence check is a lightweight operation (file system stat) compared to image processing (GPU inference), making idempotency checks extremely fast. This requirement demonstrates understanding of production system design patterns - idempotency is a fundamental principle of reliable distributed systems. For enterprise customers, this enables maintenance workflows and incremental indexing strategies.

[REQUIREMENT]
UID: REQ-014
STATEMENT: The tool shall support configurable batch processing with memory-aware batch sizing to balance processing speed and GPU memory constraints.
COMMENT: GPU batch processing requires careful memory management. Larger batches improve GPU utilization (amortizing kernel launch overhead) but risk out-of-memory (OOM) errors. Configurable batch sizing enables: (1) Hardware adaptation - different GPUs (8GB vs 24GB VRAM) require different batch sizes; (2) Image size handling - small images (1MP) can use larger batches than large images (50MP); (3) Model selection - different models (YOLOv12x vs YOLOv11-pose) have different memory footprints; (4) Performance tuning - users can optimize batch size for their specific hardware and image characteristics; (5) Error prevention - memory-aware sizing prevents OOM crashes that would require checkpoint recovery. The "memory-aware" requirement implies the system should detect available VRAM and suggest appropriate batch sizes, or at minimum validate that batch size fits available memory. This requirement demonstrates deep understanding of GPU computing constraints and production system reliability - preventing crashes is more important than maximizing theoretical throughput.

[REQUIREMENT]
UID: REQ-015
STATEMENT: The tool shall implement robust error handling to gracefully handle corrupted or unreadable images, log errors, continue processing, and report error statistics at completion with optional retry mechanisms.
COMMENT: Large collections inevitably contain corrupted, unreadable, or malformed images. Robust error handling prevents single bad images from halting entire processing runs. The requirements include: (1) Graceful degradation - corrupted images are logged and skipped, not causing crashes; (2) Continuation - processing continues after errors, ensuring maximum collection coverage; (3) Error logging - detailed error logs enable diagnosis of image issues (corruption, unsupported formats, permission errors); (4) Error statistics - summary reports enable users to identify problematic image subsets; (5) Retry mechanisms - transient errors (network timeouts, temporary I/O issues) can be retried automatically. This requirement demonstrates production system maturity - real-world data is messy, and systems must handle edge cases gracefully. For enterprise customers processing millions of images, losing processing progress due to a single corrupted image is unacceptable. The error handling approach distinguishes professional software from prototypes that assume perfect input data.

[REQUIREMENT]
UID: REQ-016
STATEMENT: The tool shall implement multi-level verbosity logging with support for verbose flags (-v through -vvvv): -vvvvv for DEBUG level (10), -vvvv for TRACE level with TQDM progress bars (12), -vvv for VERBOSE level with detailed processing info (15), -vv for DETAILED file-by-file processing (17), -v for INFO level (20), and default WARNING level (30) showing only warnings and errors. Structured logging shall be configured with file destination options.
COMMENT: Different users need different levels of detail: developers debugging issues need verbose output, while production operators need minimal noise. The multi-level verbosity system provides: (1) Progressive detail - each verbosity level adds more information, enabling users to find their preferred detail level; (2) Developer productivity - DEBUG/TRACE levels enable detailed troubleshooting without code changes; (3) Production operations - WARNING level minimizes log noise in production environments; (4) User experience - default WARNING level provides clean output for normal users, while verbose options satisfy power users; (5) Performance visibility - different levels show different performance metrics (file-level vs batch-level vs summary). The TQDM progress bars at TRACE level provide visual feedback for long-running operations. Structured logging (JSON format) enables log aggregation and analysis tools (ELK stack, Splunk) for enterprise deployments. File destination options allow logging to files for later analysis, separate from console output. This requirement demonstrates understanding of observability best practices - production systems need configurable logging appropriate for different use cases.

[REQUIREMENT]
UID: REQ-017
STATEMENT: The tool shall support configuration file (YAML/TOML) for batch processing settings with CLI argument override capability.
COMMENT: Configuration files enable reproducible processing configurations and reduce command-line complexity. The requirements include: (1) Reproducibility - configuration files enable exact reproduction of processing runs, critical for debugging and auditing; (2) Complexity management - batch processing has many options (batch size, model selection, output formats), and configuration files are more manageable than long command lines; (3) Version control - configuration files can be version-controlled, enabling tracking of processing configurations over time; (4) Team collaboration - teams can share standard configurations; (5) CLI override - argument overrides enable one-off adjustments without editing configuration files. YAML/TOML support provides flexibility (YAML is human-readable, TOML is more structured). This requirement demonstrates understanding of configuration management best practices - separating configuration from code, enabling reproducibility, and supporting both scripted and interactive usage patterns. For enterprise deployments, configuration files enable standardization across processing runs.

[REQUIREMENT]
UID: REQ-018
STATEMENT: The tool shall support image format filtering to specify which image formats to process, defaulting to common formats (JPEG, PNG, TIFF, RAW).
COMMENT: Image format filtering enables users to exclude unwanted formats or focus on specific formats. This provides: (1) Processing efficiency - skipping unwanted formats (e.g., GIF animations, WebP) saves processing time and GPU resources; (2) Use case support - users may only need to process RAW files or only JPEG files; (3) Format support - not all formats may be supported by all detection models, so filtering prevents errors; (4) Incremental processing - users can process formats separately (e.g., JPEG first, then RAW); (5) Default safety - defaulting to common formats ensures reasonable behavior for typical use cases. The explicit format list (JPEG, PNG, TIFF, RAW) covers the vast majority of photography use cases while excluding edge cases (GIF, BMP, proprietary formats) that may not be well-supported. This requirement demonstrates user-centric design - providing control while maintaining sensible defaults.

[REQUIREMENT]
UID: REQ-019
STATEMENT: The tool shall provide an optional SQLite database index for extracted metadata to enable querying by faces, objects, poses, and EXIF tags.
COMMENT: Sidecar files provide per-image metadata, but querying across millions of images requires a database index. SQLite was selected because: (1) Zero configuration - no database server required, single file database suitable for embedded use; (2) ACID compliance - ensures data integrity for concurrent access; (3) SQL support - standard SQL enables complex queries (find all images with person X AND car AND taken in 2023); (4) Portability - SQLite databases are portable across platforms; (5) Performance - SQLite handles millions of rows efficiently with proper indexing. The "optional" requirement acknowledges that some users may only need sidecar files (simpler, more portable), while others need database querying (enabling search UIs, analytics, integration with other tools). The explicit query types (faces, objects, poses, EXIF) define the use cases: person-based search, content-based search, activity-based search, and temporal/spatial queries. This requirement demonstrates understanding of different user needs - some need simple file-based metadata, others need queryable databases.

[REQUIREMENT]
UID: REQ-022
STATEMENT: The tool shall use PonyORM as the Object-Relational Mapping (ORM) framework for database operations.
COMMENT: ORM frameworks abstract database operations, enabling type-safe database access and reducing boilerplate code. PonyORM was selected over alternatives (SQLAlchemy, Django ORM, Peewee) because: (1) Pythonic syntax - PonyORM uses generator expressions for queries, resulting in more readable code than SQLAlchemy's query builder; (2) Performance - PonyORM generates efficient SQL and supports lazy loading optimizations; (3) Type safety - PonyORM models enable type checking with mypy, critical for large codebases; (4) Lightweight - PonyORM has minimal dependencies compared to Django ORM; (5) Active development - PonyORM is actively maintained with modern Python support. The ORM requirement abstracts database implementation details, enabling future migration to PostgreSQL or other databases if needed. This requirement demonstrates architectural decision-making - selecting appropriate tools based on code quality, maintainability, and performance considerations rather than defaulting to the most popular option.

[REQUIREMENT]
UID: REQ-023
STATEMENT: Each PonyORM model entity (Image, Face, Object, Pose, EXIFData) shall be defined in a separate file within the db module directory.
COMMENT: Separating model definitions into individual files follows the Single Responsibility Principle and improves code maintainability. This organization provides: (1) Modularity - each model is self-contained, making it easier to understand and modify individual entities; (2) Reduced merge conflicts - multiple developers can work on different models without conflicts; (3) Clear structure - file names map directly to model names, improving code discoverability; (4) Testability - models can be tested in isolation; (5) Scalability - adding new models doesn't bloat existing files. The explicit model list (Image, Face, Object, Pose, EXIFData) ensures all database entities are properly organized. This requirement demonstrates adherence to software engineering best practices for code organization - large files become unmaintainable, and modular structure improves long-term code quality.

[REQUIREMENT]
UID: REQ-024
STATEMENT: The database schema shall implement relational design with proper foreign key relationships: Image (1:N with Face, Object, Pose), Image (1:1 with EXIFData, ImageMetadata).
COMMENT: Relational database design ensures data integrity and enables efficient querying. The schema relationships reflect real-world data: (1) One-to-many (1:N) - one image can have multiple faces, objects, or poses detected, reflecting that images contain multiple entities; (2) One-to-one (1:1) - each image has exactly one set of EXIF data and one metadata record, reflecting that EXIF is embedded per-image; (3) Foreign key constraints - enforce referential integrity, preventing orphaned records and ensuring data consistency; (4) Query efficiency - proper relationships enable JOIN operations without complex workarounds; (5) Data normalization - avoids redundant data storage (e.g., storing image path multiple times). The explicit relationship specification ensures the schema supports efficient queries like "find all images containing person X" (Image JOIN Face) and "find all images taken in 2023" (Image JOIN EXIFData). This requirement demonstrates understanding of database design principles - proper schema design is foundational for query performance and data integrity.

[REQUIREMENT]
UID: REQ-025
STATEMENT: The tool shall support database storage mode with the --db flag specifying the database file path. When specified, metadata shall be stored in the database.
COMMENT: Database storage provides centralized querying capabilities distinct from file-based sidecar storage. The --db flag enables: (1) Opt-in functionality - users who don't need database querying can skip database creation, reducing complexity; (2) File path specification - allows users to control database location (local disk, network share, etc.); (3) Explicit activation - makes database usage intentional rather than automatic, improving user awareness; (4) Multiple databases - users can maintain separate databases for different collections or purposes. The requirement that metadata "shall be stored" when --db is specified ensures predictable behavior - if the flag is provided, database storage is guaranteed, not optional. This requirement demonstrates user-centric design - providing powerful features while maintaining simplicity for users who don't need them.

[REQUIREMENT]
UID: REQ-026
STATEMENT: The tool shall support the --no-sidecar flag to disable sidecar file generation when using database storage mode.
COMMENT: Some users prefer database-only storage for simplicity or disk space reasons. The --no-sidecar flag provides: (1) Storage efficiency - avoids duplicate metadata storage (database + sidecar files), reducing disk usage; (2) Simplified workflow - users who only query via database don't need sidecar files; (3) Performance - eliminating sidecar file I/O reduces processing overhead slightly; (4) Use case support - enterprise deployments may prefer centralized database storage over distributed sidecar files. The flag is only meaningful when --db is specified (database mode), making it a conditional option. This requirement demonstrates understanding of different user preferences - some want redundancy (database + sidecar), others want efficiency (database only). Providing both options maximizes user satisfaction.

[REQUIREMENT]
UID: REQ-027
STATEMENT: The tool shall support concurrent storage to both database and sidecar files when --db is specified without --no-sidecar.
COMMENT: Dual storage (database + sidecar) provides maximum flexibility and redundancy. This approach offers: (1) Redundancy - metadata stored in two locations provides backup and recovery options; (2) Portability - sidecar files travel with images, enabling metadata sharing without database access; (3) Tool compatibility - sidecar files enable integration with other tools that don't access the database; (4) Database migration - sidecar files provide a backup if database corruption occurs; (5) Workflow support - users can query via database while maintaining portable sidecar files. The "concurrent" requirement ensures both storage operations happen during processing, not sequentially, minimizing performance impact. This requirement demonstrates understanding of enterprise requirements - redundancy and portability are often more valuable than storage efficiency.

[REQUIREMENT]
UID: REQ-028
STATEMENT: The database schema shall be designed to support efficient querying by face embeddings, object classes, pose keypoints, and EXIF tags without requiring schema changes for new query patterns.
COMMENT: Future-proof schema design prevents costly migrations when new query patterns emerge. The requirement ensures: (1) Extensibility - schema can accommodate new query types without structural changes; (2) Index optimization - proper indexing strategy supports the specified query types efficiently; (3) Query flexibility - supports both exact matches and similarity searches (e.g., face embedding similarity); (4) Performance - schema design considers query performance, not just data storage; (5) Migration avoidance - prevents breaking changes that would require database reconstruction. The explicit query types (embeddings, classes, keypoints, tags) represent the core use cases: face similarity search, object class filtering, pose-based queries, and EXIF-based temporal/spatial queries. This requirement demonstrates forward-thinking architecture - designing for known requirements while ensuring extensibility for unknown future needs.

[REQUIREMENT]
UID: REQ-020
STATEMENT: The tool shall implement performance optimization including parallel processing with thread-based I/O operations, batch processing for GPU acceleration, and optimized default batch sizing based on available VRAM (default 4 images per batch for 12GB VRAM).
COMMENT: Performance optimization is critical for processing large collections within acceptable timeframes. The optimizations include: (1) Parallel I/O - thread-based I/O operations prevent GPU idle time while loading images from disk; (2) GPU batching - batch processing amortizes GPU kernel launch overhead and improves memory bandwidth utilization; (3) Default batch sizing - 4 images per batch for 12GB VRAM represents a conservative default that works across most modern GPUs while leaving headroom for different image sizes; (4) Memory awareness - VRAM-based sizing prevents out-of-memory errors while maximizing throughput; (5) Balanced approach - optimizes both I/O (CPU-bound) and inference (GPU-bound) operations. The explicit default (4 images for 12GB) provides a starting point that users can tune based on their specific hardware and image sizes. This requirement demonstrates performance engineering expertise - understanding that optimization requires coordination between CPU and GPU resources, not just maximizing individual component performance.

[REQUIREMENT]
UID: REQ-029
STATEMENT: The tool shall support subcommand-based CLI operation with commands: extract (extract features from images), annotate (add features to images), and convert (migrate data between sidecar and database formats).
COMMENT: Subcommand-based CLI architecture provides clear separation of concerns and intuitive user interface. The design offers: (1) Clarity - each subcommand has a single, well-defined purpose; (2) Discoverability - users can explore available commands via help system; (3) Extensibility - new functionality can be added as new subcommands without cluttering the main command; (4) Parameter isolation - each subcommand can have its own argument set without conflicts; (5) Professional UX - follows conventions established by tools like git, docker, kubectl. The three subcommands cover the core workflows: extract (initial processing), annotate (adding metadata to existing images), and convert (data migration between storage formats). This requirement demonstrates UX design thinking - recognizing that different user workflows require different command interfaces, and subcommands provide the right abstraction level.

[REQUIREMENT]
UID: REQ-030
STATEMENT: The 'extract' subcommand shall process images and extract features including faces, objects, poses, and EXIF data, storing results in sidecar files and/or database as specified by configuration.
COMMENT: The extract subcommand implements the core functionality defined in REQ-002. The explicit feature list (faces, objects, poses, EXIF) ensures completeness - all metadata types are extracted in a single pass. The storage flexibility (sidecar and/or database) reflects REQ-025 through REQ-027, allowing users to choose their preferred storage approach. This requirement establishes extract as the primary processing workflow, distinct from annotate (which adds to existing metadata) and convert (which migrates between formats). The requirement demonstrates systematic feature design - breaking complex functionality into clear, testable subcommands.

[REQUIREMENT]
UID: REQ-031
STATEMENT: The 'annotate' subcommand shall process images and add features (faces, objects, poses, EXIF data) as annotations, storing results in sidecar files and/or database as specified by configuration.
COMMENT: The annotate subcommand enables incremental metadata updates without full reprocessing. This provides: (1) Model updates - when new detection models are released, users can re-annotate with new models without reprocessing EXIF data; (2) Selective processing - users can add specific metadata types (e.g., only faces) to existing images; (3) Manual corrections - enables adding manually curated annotations alongside AI-detected features; (4) Cost efficiency - avoids reprocessing expensive operations (EXIF extraction) when only detection models need updating; (5) Workflow support - supports workflows where metadata is added incrementally over time. The distinction from extract (which processes everything) provides users with fine-grained control over processing workflows. This requirement demonstrates understanding of real-world usage patterns - initial processing is different from ongoing maintenance and updates.

[REQUIREMENT]
UID: REQ-032
STATEMENT: The 'convert' subcommand shall support importing existing sidecar files into the database, allowing migration from sidecar-only storage to database storage.
COMMENT: Migration from sidecar-only to database storage enables users to adopt querying capabilities for existing collections. This provides: (1) Incremental adoption - users can start with sidecar files and migrate to database when querying needs arise; (2) Workflow evolution - supports user workflows that evolve from simple file-based to query-based; (3) Data consolidation - enables combining metadata from multiple sidecar files into a single queryable database; (4) Tool integration - migration enables integration with database-querying tools and UIs; (5) Backup and recovery - database import can recreate databases from sidecar file backups. The import operation validates sidecar file format and handles errors gracefully, ensuring data integrity during migration. This requirement demonstrates understanding of user migration needs - users don't start with perfect workflows, and tools must support evolution of usage patterns.

[REQUIREMENT]
UID: REQ-033
STATEMENT: The 'convert' subcommand shall support exporting database contents to sidecar files, allowing extraction of database-stored features to individual sidecar JSON files.
COMMENT: Database-to-sidecar export enables reverse migration and metadata portability. This provides: (1) Portability - users can extract metadata from databases to portable sidecar files for sharing or backup; (2) Tool compatibility - enables use of sidecar-compatible tools even when metadata is stored in databases; (3) Backup strategy - sidecar files provide backup format independent of database technology; (4) Distributed workflows - sidecar files can be shared across systems without database access; (5) Migration flexibility - supports bidirectional migration between storage formats. The export operation ensures each image gets its corresponding sidecar file, maintaining the one-to-one relationship between images and metadata files. This requirement demonstrates understanding of data portability needs - users need flexibility to move between storage formats as requirements evolve.

[REQUIREMENT]
UID: REQ-034
STATEMENT: The 'convert' subcommand shall provide --direction flag with values 'to-db' (import sidecar to database) and 'to-sidecar' (export database to sidecar).
COMMENT: Explicit direction specification prevents ambiguity and makes conversion operations clear and intentional. The flag design provides: (1) Clarity - direction is explicit, not inferred from arguments or context; (2) Safety - prevents accidental conversion in wrong direction; (3) Discoverability - users can see available conversion directions via help; (4) Extensibility - new directions (e.g., 'to-json', 'to-csv') can be added without breaking existing usage; (5) Validation - direction flag enables validation of required arguments for each direction. The two values ('to-db', 'to-sidecar') cover bidirectional conversion between the two primary storage formats. This requirement demonstrates UX design thinking - making operations explicit and discoverable reduces user errors and improves usability.

[REQUIREMENT]
UID: REQ-035
STATEMENT: The tool shall maintain backwards compatibility with legacy CLI usage where subcommands are not explicitly specified, automatically defaulting to the extract subcommand.
COMMENT: Backwards compatibility protects existing user workflows and scripts from breaking when CLI architecture evolves. This provides: (1) User protection - existing scripts and workflows continue to work without modification; (2) Gradual migration - users can adopt subcommand syntax at their own pace; (3) Simplicity - default behavior (extract) matches most common use case; (4) Script compatibility - shell scripts and automation don't require immediate updates; (5) User experience - maintains familiar command syntax for existing users. The default to extract reflects that extraction is the primary workflow (REQ-002), making the default behavior intuitive. This requirement demonstrates understanding of production system evolution - breaking changes hurt users, and backwards compatibility is essential for professional software.

[REQUIREMENT]
UID: REQ-036
STATEMENT: All Python functions and files SHALL be documented to meet NumPy documentation standards as specified in the NumPy documentation style guide.
COMMENT: Consistent documentation standards improve code maintainability and developer productivity. NumPy docstring format was selected because: (1) Industry standard - NumPy format is widely recognized and tool-supported; (2) Structured format - sections (Parameters, Returns, Examples) provide consistent organization; (3) Tool integration - documentation generators (Sphinx, pydoc) support NumPy format natively; (4) Type information - NumPy format accommodates type hints and parameter descriptions; (5) Examples - NumPy format encourages code examples in docstrings. The requirement applies to all functions and files, ensuring comprehensive documentation coverage. This requirement demonstrates commitment to code quality - well-documented code is maintainable, reduces onboarding time, and enables effective code reviews. For investors, comprehensive documentation signals professional development practices.

[REQUIREMENT]
UID: REQ-037
STATEMENT: Python dependency management shall use uv for installation of all packages. The virtual environment creation process shall install uv using pip, and subsequent package installations shall use 'uv pip install'.
COMMENT: uv is a modern Python package manager written in Rust that provides 10-100x faster package installation than pip. The selection rationale includes: (1) Performance - faster installation improves developer productivity and CI/CD pipeline speed; (2) Modern design - uv uses Rust for performance and implements modern dependency resolution algorithms; (3) Compatibility - uv pip install is compatible with pip's interface, minimizing migration effort; (4) Reproducibility - uv provides deterministic dependency resolution; (5) Future-proofing - uv represents the future direction of Python packaging. The two-stage installation (pip installs uv, then uv installs packages) enables bootstrapping uv without requiring pre-installation while leveraging uv's performance for actual package management. This requirement demonstrates adoption of modern tooling and performance-conscious decision-making.

[REQUIREMENT]
UID: REQ-038
STATEMENT: All imports in the CLI module shall be lazy-loaded (imported only when the specific command is executed) to minimize startup time for the media-indexer command. Imports of heavy dependencies (torch, ultralytics, image-sidecar-rust) shall not occur until the command actually needs them.
COMMENT: CLI startup time directly impacts user experience and perceived performance. Lazy loading provides: (1) Fast startup - CLI commands appear instantly, even when heavy dependencies aren't needed; (2) Better UX - users can run help commands without waiting for GPU libraries to load; (3) Resource efficiency - heavy dependencies (PyTorch, CUDA) only load when actually needed; (4) Error handling - dependency errors surface only when relevant commands are executed; (5) Development experience - faster iteration cycles during CLI development. The explicit mention of heavy dependencies (torch, ultralytics, image-sidecar-rust) identifies the specific performance bottlenecks being addressed. This requirement demonstrates attention to user experience details - fast startup times are often overlooked but significantly impact perceived quality.

[REQUIREMENT]
UID: REQ-039
STATEMENT: The tool shall support a --limit flag to process only a specified number of images, enabling testing on small subsets before processing large image collections.
COMMENT: Testing and validation on large-scale systems requires the ability to operate on sample datasets. The --limit flag provides: (1) Development efficiency - developers can test changes on small datasets (e.g., 10 images) before full runs; (2) Cost control - cloud GPU users can validate processing on small samples before committing to expensive full runs; (3) Debugging - isolated issues can be reproduced on manageable datasets; (4) Validation - users can verify processing behavior on samples before processing entire collections; (5) Performance testing - enables benchmarking on controlled dataset sizes. This requirement demonstrates understanding of software development workflows - large-scale systems require testing strategies that don't require full-scale runs for every change. For enterprise customers, the ability to validate on samples reduces operational risk.

[REQUIREMENT]
UID: REQ-040
STATEMENT: The tool shall support RAW image formats (CR2, NEF, ARW, DNG, etc.) by converting them to temporary JPEG files in memory for YOLO processing using the rawpy library.
COMMENT: Professional photographers often work with RAW formats that contain more image data than JPEG but aren't directly processable by YOLO models. RAW support provides: (1) Market coverage - supports professional photography workflows where RAW is standard; (2) Image quality - RAW files contain uncompressed sensor data, enabling better detection accuracy; (3) Format diversity - supports multiple RAW formats (Canon CR2, Nikon NEF, Sony ARW, Adobe DNG) covering major camera manufacturers; (4) Processing compatibility - converts RAW to JPEG format expected by YOLO models; (5) Memory efficiency - in-memory conversion avoids disk I/O overhead. The rawpy library provides robust RAW file parsing with support for all major formats. The "temporary" and "in memory" specifications ensure no disk space is consumed for intermediate files. This requirement demonstrates understanding of professional photography workflows and commitment to supporting industry-standard formats.

[REQUIREMENT]
UID: REQ-041
STATEMENT: All StrictDoc (.sdoc) documents SHALL follow the StrictDoc grammar rules as defined at https://github.com/strictdoc-project/strictdoc/blob/main/strictdoc/backend/sdoc/grammar/grammar.py
COMMENT: Grammar compliance ensures that StrictDoc documents are valid and can be processed by tools. This requirement: (1) Ensures validity - documents that don't follow grammar rules cannot be parsed or exported; (2) Tool compatibility - grammar-compliant documents work with all StrictDoc tools and future versions; (3) Automation support - enables automated validation in CI/CD pipelines; (4) Standardization - ensures consistency across all documentation; (5) Reference clarity - linking to the grammar source enables developers to understand exact requirements. The explicit URL reference ensures the requirement points to the authoritative grammar definition, preventing ambiguity. This requirement complements REQ-001 (StrictDoc format requirement) by specifying the exact grammar rules that must be followed.

[REQUIREMENT]
UID: REQ-042
STATEMENT: All StrictDoc (.sdoc) documents SHALL not have C-style comments at the top of the file.
COMMENT: C-style comments (/* */) are not part of the StrictDoc grammar and would cause parsing errors. This requirement ensures: (1) Grammar compliance - documents follow StrictDoc syntax rules without mixing comment styles; (2) Tool compatibility - StrictDoc parsers expect pure StrictDoc syntax without C-style comments; (3) Consistency - all documentation uses consistent comment syntax; (4) Parsing reliability - eliminates syntax ambiguity that could cause parsing failures; (5) Professional appearance - clean syntax without mixed comment styles presents a professional codebase. This requirement reflects understanding of language grammar constraints - mixing syntax styles breaks parsers and reduces tool compatibility.

[REQUIREMENT]
UID: REQ-068
STATEMENT: All StrictDoc (.sdoc) documents SHALL have the tool (Cursor.AI) and model (Claude) listed in any Git commit message involving a StrictDoc.
COMMENT: Attribution and auditability are critical for compliance and traceability in professional software development. This requirement ensures: (1) Tool transparency - commits clearly indicate when AI-assisted tools were used; (2) Model tracking - enables tracking of which AI models contributed to documentation; (3) Audit trail - provides complete history of who/what modified documentation; (4) Compliance - supports requirements for documenting AI tool usage in some regulated industries; (5) Reproducibility - enables understanding of tool/model context for documentation changes. The explicit mention of "Cursor.AI" and "Claude" ensures specific attribution rather than generic "AI assistance" claims. This requirement demonstrates commitment to transparency and auditability - critical for enterprise customers and regulatory compliance.

[REQUIREMENT]
UID: REQ-043
STATEMENT: All StrictDoc (.sdoc) documents SHALL be validated using the strictdoc CLI tool to ensure grammar compliance.
COMMENT: Automated validation prevents invalid documents from entering the codebase and ensures grammar compliance. This requirement: (1) Prevents errors - catches syntax errors before they cause problems in CI/CD or documentation generation; (2) Enforces standards - ensures all documentation follows the same grammar rules; (3) CI/CD integration - enables automated validation in pre-commit hooks and CI pipelines; (4) Developer feedback - provides immediate feedback when documentation doesn't comply; (5) Quality assurance - ensures documentation can be processed by all StrictDoc tools. The validation requirement complements REQ-001 and REQ-041 by adding enforcement - not just specifying the format, but ensuring it's followed. This requirement demonstrates commitment to quality - automated validation prevents human error and ensures consistent documentation quality.

[REQUIREMENT]
UID: REQ-044
STATEMENT: ALL notebooks must be generated programmatically from a .py script. The .py script shall have the same name as the .ipynb notebook and shall be the golden master for the notebook. If the notebooks need to be regenerated they are regenerated from the .py script.
COMMENT: Notebooks (Jupyter/IPython) are difficult to version control due to JSON metadata and cell output. Using Python scripts as the source of truth provides: (1) Version control - Python scripts are clean, diff-friendly text files; (2) Reproducibility - scripts can be run to regenerate notebooks with identical results; (3) Code quality - Python scripts can be linted, type-checked, and tested like regular code; (4) Automation - notebooks can be regenerated automatically in CI/CD pipelines; (5) Collaboration - avoids merge conflicts from notebook JSON metadata. The "golden master" concept ensures scripts are the authoritative source, preventing divergence between scripts and notebooks. This requirement demonstrates understanding of data science workflow challenges and commitment to reproducible research practices.

[REQUIREMENT]
UID: REQ-045
STATEMENT: All code changes SHALL have an accompanying requirement in the requirements documentation. Requirements SHALL be tracked in StrictDoc format. When implementing new features or functionality, ensure the corresponding requirement exists or create it. All code MUST trace back to at least one requirement.
COMMENT: Requirements traceability ensures that all code is justified by documented requirements, preventing scope creep and maintaining alignment with business goals. This requirement: (1) Prevents gold-plating - code must be justified by requirements, preventing unnecessary features; (2) Enables auditability - reviewers can verify code matches requirements; (3) Supports change management - requirement changes drive code changes, not the reverse; (4) Maintains focus - ensures development stays aligned with documented goals; (5) Professional practice - traceability is standard in regulated industries and enterprise software. The "MUST trace back" language is stronger than "SHALL" to emphasize the criticality of this practice. This requirement demonstrates mature software engineering practices - requirements-driven development distinguishes professional software companies from ad-hoc development.

[REQUIREMENT]
UID: REQ-046
STATEMENT: The git user SHALL be changed to "<username> | Cursor.sh | <model>" where <username> is the output of the whoami command. This way any git commit changed by you will be known as automated.
COMMENT: Transparent attribution is essential when AI tools assist with code generation. This requirement ensures: (1) AI transparency - commits clearly indicate AI-assisted development; (2) Audit trail - enables tracking of human vs AI contributions; (3) Compliance - supports requirements for documenting AI tool usage; (4) Attribution accuracy - preserves human username while indicating AI involvement; (5) Tool identification - specifies both IDE (Cursor.sh) and model for complete context. The format "<username> | Cursor.sh | <model>" provides clear, parsable attribution that distinguishes AI-assisted commits from purely human commits. This requirement demonstrates commitment to transparency and ethical AI use - critical for maintaining trust with stakeholders and complying with potential future regulations.

[REQUIREMENT]
UID: REQ-047
STATEMENT: ALL git commits SHALL be followed by a git push if and only if a remote exists.
COMMENT: Automated git push ensures that commits are immediately backed up to remote repositories, preventing data loss from local failures. The requirement: (1) Prevents data loss - commits are backed up immediately, not dependent on manual push; (2) Enables collaboration - team members see changes immediately after commits; (3) CI/CD integration - automated pushes trigger CI/CD pipelines automatically; (4) Backup strategy - remote repositories serve as backup for local work; (5) Conditional safety - "if and only if a remote exists" prevents errors when no remote is configured. The automatic push requirement reflects understanding that manual processes are error-prone and automation improves reliability. This requirement demonstrates DevOps maturity - automation reduces manual steps and prevents human error.

[REQUIREMENT]
UID: REQ-048
STATEMENT: The commit message SHALL include the filename and what changed / why it was added. In addition to a summary of the prompt as to why the changes were made.
COMMENT: Comprehensive commit messages enable effective code review, debugging, and project history understanding. This requirement ensures: (1) Code review efficiency - reviewers understand what changed and why without reading diffs; (2) Historical context - commit history serves as project documentation; (3) Debugging aid - detailed messages help identify when and why bugs were introduced; (4) Change tracking - enables tracking of feature evolution over time; (5) Prompt context - for AI-assisted development, prompt context explains the reasoning behind changes. The dual requirement (filename/what changed + prompt summary) provides both technical details and business context. This requirement demonstrates commitment to code quality - good commit messages are essential for maintainable codebases.

[REQUIREMENT]
UID: REQ-049
STATEMENT: ALWAYS pull from remote before committing changes.
COMMENT: Pulling before committing prevents merge conflicts and ensures code is based on the latest remote state. This requirement: (1) Prevents conflicts - pulling first integrates remote changes before local commits; (2) Ensures consistency - local work is based on latest remote state; (3) Reduces merge complexity - small, frequent pulls prevent large merge conflicts; (4) Collaboration safety - ensures local changes don't conflict with concurrent work; (5) Best practice - pulling before committing is standard git workflow. The "ALWAYS" emphasis (uppercase) indicates this is a critical practice, not optional. This requirement demonstrates understanding of distributed version control best practices - preventing conflicts is more efficient than resolving them.

[REQUIREMENT]
UID: REQ-050
STATEMENT: Never under any circumstances SHALL rm -rf be run on ~/ (the user's home directory).
COMMENT: Preventing deletion of user home directories is a critical safety requirement to avoid catastrophic data loss. This requirement: (1) Prevents data loss - home directories contain user data that cannot be recovered; (2) Safety override - even if other commands suggest home directory deletion, this requirement takes precedence; (3) User protection - protects users from accidental or malicious destruction of their data; (4) Legal protection - prevents liability from data destruction; (5) Professional practice - responsible software never risks user data destruction. The "Never under any circumstances" language is absolute to prevent any exceptions. This requirement demonstrates commitment to user safety - protecting user data is paramount, even at the cost of functionality.

[REQUIREMENT]
UID: REQ-051
STATEMENT: Under no circumstances should sudo be run. All commands SHALL be done in userspace.
COMMENT: Avoiding sudo prevents privilege escalation and reduces security risks. This requirement: (1) Security - prevents accidental privilege escalation that could compromise system security; (2) User safety - avoids requiring root access for normal operations; (3) Portability - userspace operations work without special permissions; (4) Error prevention - prevents accidental system-wide changes; (5) Best practice - following principle of least privilege. The "under no circumstances" language prevents exceptions that could compromise security. This requirement demonstrates security-conscious development practices - minimizing privilege requirements reduces attack surface and operational risk.

[REQUIREMENT]
UID: REQ-052
STATEMENT: The exclamation mark (!) SHALL NOT be used in strings meant for the command line / bash.
COMMENT: Exclamation marks have special meaning in bash (history expansion) and can cause unexpected behavior when used in strings. This requirement: (1) Prevents errors - avoids bash history expansion triggering unintended commands; (2) Portability - ensures commands work consistently across different shell configurations; (3) Predictability - prevents shell-specific behavior from affecting command execution; (4) Safety - avoids potential command injection risks from history expansion; (5) Compatibility - ensures commands work with history expansion disabled. This requirement demonstrates attention to shell scripting details - understanding subtle shell behaviors prevents hard-to-debug issues.

[REQUIREMENT]
UID: REQ-053
STATEMENT: The system pip SHALL NOT be used. All Python related matters SHALL use a virtual environment.
COMMENT: Isolating Python dependencies prevents conflicts with system packages and other projects. This requirement: (1) Prevents conflicts - virtual environments isolate dependencies from system Python; (2) Reproducibility - virtual environments enable reproducible builds across systems; (3) Safety - avoids modifying system Python that other software depends on; (4) Professional practice - virtual environments are standard for Python development; (5) Portability - projects can be moved between systems without system modification. The prohibition on system pip ensures all package management goes through virtual environments. This requirement demonstrates Python best practices - isolation is fundamental to reliable Python development.

[REQUIREMENT]
UID: REQ-054
STATEMENT: Python packages SHALL NOT be installed to the system. All Python related matters SHALL use a virtual environment.
COMMENT: Installing packages to system Python can break system tools and create conflicts. This requirement: (1) System stability - prevents breaking system Python packages that OS tools depend on; (2) Clean separation - clearly separates project dependencies from system packages; (3) Uninstall safety - project removal doesn't affect system Python; (4) Multi-project support - enables multiple projects with different dependency versions; (5) Professional practice - system package installation is considered harmful in Python development. This requirement reinforces REQ-053 by explicitly prohibiting system installation, ensuring virtual environment usage. This requirement demonstrates commitment to proper Python dependency management - a fundamental skill for professional Python development.

[REQUIREMENT]
UID: REQ-055
STATEMENT: All Python related matters SHALL use a virtual environment. The Python virtual environment SHALL be called `venv`
COMMENT: Standardizing virtual environment name improves consistency and tooling integration. This requirement: (1) Consistency - all documentation and scripts reference the same environment name; (2) Tool integration - Makefiles and scripts can reliably reference venv/bin; (3) .gitignore - standard name enables proper .gitignore patterns; (4) Developer familiarity - 'venv' is the standard name recognized by Python developers; (5) Simplicity - single name reduces cognitive load compared to project-specific names. The explicit name requirement ensures all tooling (Makefiles, CI/CD, documentation) can reference the same environment. This requirement demonstrates attention to developer experience - consistent naming reduces confusion and enables tooling.

[REQUIREMENT]
UID: REQ-056
STATEMENT: All Makefile references to pip and python installed tools like pytest SHALL use the full path to the venv bin directory, eg `venv/bin/pytest`, `venv/bin/pip`
COMMENT: Explicit paths in Makefiles ensure tools are executed from the virtual environment, not system Python. This requirement: (1) Reliability - prevents accidental use of system Python or wrong virtual environment; (2) Clarity - makes virtual environment usage explicit in Makefiles; (3) Portability - works regardless of PATH configuration; (4) Debugging - easier to identify which Python/tools are being used; (5) CI/CD compatibility - ensures consistent behavior in automated environments. The explicit path requirement complements REQ-055 (standard venv name) - with a standard name, paths can be hardcoded reliably. This requirement demonstrates Makefile best practices - explicit dependencies and paths prevent subtle bugs.

[REQUIREMENT]
UID: REQ-057
STATEMENT: All python language scripts SHALL use full mypy typing.
COMMENT: Type annotations improve code quality, enable static analysis, and enhance IDE support. This requirement: (1) Code quality - type annotations catch errors before runtime; (2) IDE support - enables better autocomplete and error detection in IDEs; (3) Documentation - types serve as inline documentation; (4) Refactoring safety - type checkers enable safe refactoring; (5) Team collaboration - types clarify function contracts for team members. "Full mypy typing" means type annotations throughout code, not just function signatures. This requirement demonstrates commitment to code quality - type annotations are essential for maintainable Python codebases, especially as codebases grow.

[REQUIREMENT]
UID: REQ-058
STATEMENT: All python language scripts SHALL be documented such that they meet numpy style guidelines.
COMMENT: Consistent documentation style improves code readability and maintainability. This requirement: (1) Consistency - all code follows the same documentation format; (2) Tool support - NumPy format is supported by documentation generators; (3) Readability - structured format improves readability of complex functions; (4) Standards compliance - NumPy format is an industry-standard Python docstring format; (5) Completeness - NumPy format encourages comprehensive documentation. This requirement complements REQ-036 and REQ-059 by emphasizing documentation style consistency. The requirement demonstrates commitment to documentation quality - good documentation is essential for code maintainability.

[REQUIREMENT]
UID: REQ-059
STATEMENT: ALL functions and files in Python SHALL be documented to meet numpy documentation standards.
COMMENT: Comprehensive documentation coverage ensures all code is understandable and maintainable. This requirement: (1) Completeness - ensures no undocumented code exists; (2) Onboarding - new developers can understand code through documentation; (3) Maintenance - documentation enables long-term code maintenance; (4) Quality assurance - undocumented code is a code smell indicating rushed development; (5) Professional practice - comprehensive documentation is standard in professional codebases. The "ALL" emphasis indicates this is a blanket requirement with no exceptions. This requirement demonstrates commitment to code quality - documentation is not optional, it's essential for maintainable software.

[REQUIREMENT]
UID: REQ-060
STATEMENT: Always include a detailed technical attribution comment at the top of each commit message with a specified format including the exact LLM model version identifier (e.g., claude-3-5-sonnet-20241022, gpt-5, etc.), IDE information, generation method, code style, and dependencies. The model identifier MUST be the specific version, not a generic name like "Claude" or "ChatGPT".
COMMENT: Detailed attribution enables tracking of AI tool usage and supports reproducibility. This requirement: (1) Complete attribution - provides full context of AI assistance including model version, IDE, and method; (2) Reproducibility - exact model versions enable reproducing results; (3) Audit trail - comprehensive attribution supports compliance and auditing; (4) Tool evolution - tracking model versions enables understanding of AI tool improvements over time; (5) Transparency - detailed attribution demonstrates commitment to transparency. The requirement for specific version identifiers (not generic names) ensures accurate tracking - different model versions have different capabilities and behaviors. This requirement demonstrates commitment to ethical AI use and transparency - critical for maintaining trust in AI-assisted development.

[REQUIREMENT]
UID: REQ-061
STATEMENT: After all file changes add and commit.
COMMENT: Automatic commit ensures changes are tracked and not lost. This requirement: (1) Version control - ensures all changes are captured in git history; (2) Backup - commits provide backup of work in progress; (3) Rollback capability - committed changes can be reverted if needed; (4) Collaboration - enables sharing changes with team members; (5) Workflow consistency - ensures consistent git workflow. The "after all file changes" timing ensures commits are atomic and complete. This requirement demonstrates understanding of version control best practices - frequent commits prevent work loss and enable collaboration.

[REQUIREMENT]
UID: REQ-062
STATEMENT: Makefiles SHALL use full escape for all variables: ${VAR} not $VAR to avoid confusion.
COMMENT: Explicit variable syntax prevents ambiguity and ensures correct Makefile expansion. This requirement: (1) Clarity - ${VAR} is unambiguous, $VAR can be confused with shell variables; (2) Reliability - prevents accidental shell variable expansion; (3) Portability - works consistently across different make implementations; (4) Debugging - explicit syntax makes variable usage clear; (5) Best practice - ${VAR} is the recommended Makefile syntax. This requirement demonstrates attention to Makefile details - subtle syntax differences can cause hard-to-debug issues.

[REQUIREMENT]
UID: REQ-063
STATEMENT: All makefiles SHALL use an actual file/folder where possible as a target. For example python -mvenv venv generates venv. Any target that depends on Python having the venv built shall use `venv` as a dependency so that the GNU make resolves everything correctly and in the right order.
COMMENT: Using file-based targets enables Make's dependency resolution to work correctly. This requirement: (1) Dependency resolution - Make can determine if targets need rebuilding based on file timestamps; (2) Correct ordering - dependencies ensure targets build in correct order; (3) Efficiency - Make skips unnecessary rebuilds when dependencies are up-to-date; (4) Reliability - file-based targets prevent race conditions and incorrect builds; (5) Best practice - file-based targets are the standard Make pattern. The example (venv directory as target) illustrates the principle - Make can check if venv exists and rebuild only if needed. This requirement demonstrates deep understanding of Make - proper dependency management is essential for reliable builds.

[REQUIREMENT]
UID: REQ-064
STATEMENT: The tool shall provide a CLI command to emit a default configuration file with all available options and their descriptions, enabling users to understand the syntax and use it as a template without creating configuration files from scratch.
COMMENT: Configuration file generation improves user experience by eliminating guesswork. This requirement: (1) Discoverability - users can see all available options without reading documentation; (2) Template generation - provides starting point for custom configurations; (3) Documentation - emitted config serves as inline documentation; (4) Error prevention - reduces syntax errors by providing correct template; (5) User experience - eliminates need to create config files manually. The requirement complements REQ-017 (configuration file support) by enabling easy configuration file creation. This requirement demonstrates user-centric design - making tools easy to use is as important as making them powerful.

[REQUIREMENT]
UID: REQ-065
STATEMENT: When determining the model version for commit attribution, the system SHALL use the most specific available model identifier. If an exact version identifier is available (e.g., from CURSOR_MODEL environment variable or system information), that specific version SHALL be used. Otherwise, the best available approximation SHALL be used, but MUST distinguish between different model families (e.g., Claude Sonnet 3.5, ChatGPT-5, GPT-4, etc.).
COMMENT: Accurate model attribution requires precise version identification, not generic names. This requirement: (1) Accuracy - specific versions enable exact reproduction of AI-assisted work; (2) Traceability - precise version tracking enables understanding of model capabilities; (3) Compliance - supports requirements for documenting AI tool usage; (4) Reproducibility - exact versions enable reproducing results; (5) Honesty - prevents misleading attribution with generic names. The hierarchy (exact version > approximation > family) ensures best available accuracy while preventing false precision. This requirement demonstrates commitment to accurate attribution - transparent AI use requires precise model identification.

[REQUIREMENT]
UID: REQ-066
STATEMENT: When database storage is enabled with the --db flag, the database SHALL be properly initialized with all required tables (Image, Face, Object, Pose, EXIFData) before any metadata storage operations are performed. If database initialization fails or tables are not created, the tool SHALL raise an error and NOT proceed with processing.
COMMENT: Database initialization validation prevents data corruption and ensures schema consistency. This requirement: (1) Data integrity - ensures schema exists before data insertion; (2) Error prevention - catches initialization failures early, before processing starts; (3) Reliability - guarantees all required tables exist before use; (4) Debugging - early failure provides clear error messages vs cryptic database errors later; (5) Safety - prevents partial processing that could corrupt data. The explicit table list (Image, Face, Object, Pose, EXIFData) ensures all required entities are verified. The "NOT proceed" requirement is absolute - processing must stop if initialization fails. This requirement demonstrates understanding of database reliability - schema must be verified before use.

[REQUIREMENT]
UID: REQ-067
STATEMENT: Database management operations SHALL be organized under a 'db' command group with subcommands: 'stats' (display database statistics), 'init' (initialize database), 'search' (search for images), and 'clean' (remove orphaned records). The CLI syntax SHALL be 'media-indexer db <subcommand>'.
COMMENT: Organizing database operations under a command group provides clear separation and intuitive CLI structure. This requirement: (1) Organization - groups related database operations together; (2) Discoverability - users can explore database commands via 'db --help'; (3) Consistency - follows established CLI patterns (git, docker, kubectl); (4) Extensibility - new database operations can be added as subcommands; (5) User experience - clear command structure improves usability. The subcommands cover essential database operations: stats (monitoring), init (setup), search (querying), and clean (maintenance). This requirement demonstrates UX design thinking - organizing commands logically improves discoverability and usability.

[REQUIREMENT]
UID: REQ-069
STATEMENT: Output suppression utilities SHALL provide context-managed redirection of both stdout and stderr so that verbosity controls (REQ-016) silence third-party libraries only within the intended scope without leaking file descriptors or mutating global handlers.
COMMENT: Reliable suppression is essential for the CLI logging contract. If stdout/stderr are not redirected together, noisy native dependencies (OpenCV, ONNX Runtime) can still spam the console, undermining REQ-016. Proper use of context managers eliminates global side effects, guarantees deterministic teardown even on exceptions, and prevents descriptor leaks that can exhaust process resources during long-running jobs.

[REQUIREMENT]
UID: REQ-070
STATEMENT: Parallel execution helpers SHALL return structured result records capturing the original work item, success flag, optional payload, and any raised exception to support robust error handling (REQ-015) while preserving type safety.
COMMENT: Returning bare tuples with sentinel ``None`` values obscures failures and defeats static typing (REQ-057). Structured results make it trivial to inspect and log failures, avoid loss of diagnostic context, and keep the parallel orchestration layer aligned with the project's strict typing guarantees. This design also future-proofs the executor for richer metadata (durations, retries) without breaking callers.

[REQUIREMENT]
UID: REQ-071
STATEMENT: Model downloads SHALL stream payloads to a temporary file and atomically promote the completed file into place, ensuring resilient delivery of large model artifacts mandated by REQ-007, REQ-008, and REQ-009.
COMMENT: ``urllib.request.urlretrieve`` buffers whole files in-memory and provides weak error handling, which risks partial downloads and corrupted weights. Streaming to a temporary file guarantees bounded memory usage, enables retry-friendly cleanup, and prevents exposing half-written models if a transfer fails. Atomic promotion ensures consumers either observe the previous good version or the newly downloaded file, never a torn write.

[REQUIREMENT]
UID: REQ-072
STATEMENT: All version-controlled text files SHALL contain no more than 500 lines, and a git pre-commit hook SHALL block commits that stage files exceeding this limit.
COMMENT: Limiting file length enforces modular design and maintainability by encouraging developers to decompose oversized modules before committing changes. The automated pre-commit guard provides immediate feedback, preventing regressions as the codebase evolves. This aligns with professional engineering practices by ensuring architectural discipline, simplifying code reviews, and avoiding technical debt that accrues when monolithic files accumulate unnoticed.

